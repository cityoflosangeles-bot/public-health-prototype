{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stitch JHU data through various schema changes\n",
    "* Reshape\n",
    "* See what columns we need to derive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/rogerallen/1583593\n",
    "us_state_abbrev = {\n",
    "    'Alabama': 'AL',\n",
    "    'Alaska': 'AK',\n",
    "    'American Samoa': 'AS',\n",
    "    'Arizona': 'AZ',\n",
    "    'Arkansas': 'AR',\n",
    "    'California': 'CA',\n",
    "    'Colorado': 'CO',\n",
    "    'Connecticut': 'CT',\n",
    "    'Delaware': 'DE',\n",
    "    'District of Columbia': 'DC',\n",
    "    'Florida': 'FL',\n",
    "    'Georgia': 'GA',\n",
    "    'Guam': 'GU',\n",
    "    'Hawaii': 'HI',\n",
    "    'Idaho': 'ID',\n",
    "    'Illinois': 'IL',\n",
    "    'Indiana': 'IN',\n",
    "    'Iowa': 'IA',\n",
    "    'Kansas': 'KS',\n",
    "    'Kentucky': 'KY',\n",
    "    'Louisiana': 'LA',\n",
    "    'Maine': 'ME',\n",
    "    'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA',\n",
    "    'Michigan': 'MI',\n",
    "    'Minnesota': 'MN',\n",
    "    'Mississippi': 'MS',\n",
    "    'Missouri': 'MO',\n",
    "    'Montana': 'MT',\n",
    "    'Nebraska': 'NE',\n",
    "    'Nevada': 'NV',\n",
    "    'New Hampshire': 'NH',\n",
    "    'New Jersey': 'NJ',\n",
    "    'New Mexico': 'NM',\n",
    "    'New York': 'NY',\n",
    "    'North Carolina': 'NC',\n",
    "    'North Dakota': 'ND',\n",
    "    'Northern Mariana Islands':'MP',\n",
    "    'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR',\n",
    "    'Pennsylvania': 'PA',\n",
    "    'Puerto Rico': 'PR',\n",
    "    'Rhode Island': 'RI',\n",
    "    'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD',\n",
    "    'Tennessee': 'TN',\n",
    "    'Texas': 'TX',\n",
    "    'Utah': 'UT',\n",
    "    'Vermont': 'VT',\n",
    "    'Virgin Islands': 'VI',\n",
    "    'Virginia': 'VA',\n",
    "    'Washington': 'WA',\n",
    "    'West Virginia': 'WV',\n",
    "    'Wisconsin': 'WI',\n",
    "    'Wyoming': 'WY',\n",
    "    # Add some other ones we found applicable\n",
    "    'US Virgin Islands': 'VI', \n",
    "    'United States Virgin Islands': 'VI',\n",
    "    'Grand Princess': 'Grand Princess',\n",
    "    'Diamond Princess': 'Diamond Princess', \n",
    "    'From Diamond Princess': 'Diamond Princess', \n",
    "    'Diamond Princess cruise ship': 'Diamond Princess'\n",
    "}\n",
    "\n",
    "# reverse the dict\n",
    "abbrev_us_state = dict(map(reversed, us_state_abbrev.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre 2/14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre214_cases_url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/archived_data/archived_time_series/time_series_2019-ncov-Confirmed.csv\"\n",
    "pre214_deaths_url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/archived_data/archived_time_series/time_series_2019-ncov-Deaths.csv\"\n",
    "pre214_recovered_url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/archived_data/archived_time_series/time_series_2019-ncov-Recovered.csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases1 = pd.read_csv(pre214_cases_url)\n",
    "deaths1 = pd.read_csv(pre214_deaths_url)\n",
    "recovered1 = pd.read_csv(pre214_recovered_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified helper function, since columns are datetime, will extract date portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_columns(df):\n",
    "    \"\"\"\n",
    "    quick helper function to parse columns into values\n",
    "    uses for pd.melt\n",
    "    \"\"\"\n",
    "    df.columns = df.columns.str.split(' ').str[0]\n",
    "    columns = list(df.columns)\n",
    "    id_vars, dates = [], []\n",
    "\n",
    "    for c in columns:\n",
    "        if c.endswith(\"20\"):\n",
    "            dates.append(c)\n",
    "        else:\n",
    "            id_vars.append(c)\n",
    "    return id_vars, dates\n",
    "\n",
    "# Rename geography columns to be the same as future schemas\n",
    "def rename_geog_cols(df):\n",
    "    df.rename(columns = {'Country/Region':'Country_Region', \n",
    "                         'Province/State': 'Province_State', \n",
    "                         'Long': 'Lon'}, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some functions we'll use to get totals\n",
    "# Calculate US State totals\n",
    "def us_state_totals(df):\n",
    "    \n",
    "    state_grouping_cols = ['Country_Region', 'state_abbrev', 'date']\n",
    "    \n",
    "    state_totals = df.groupby(state_grouping_cols).agg(\n",
    "        {'cases':'sum', 'recovered':'sum', 'deaths':'sum'})\n",
    "    \n",
    "    state_totals.rename(columns = {'cases': 'state_cases',\n",
    "                                  'recovered':'state_recovered', \n",
    "                                  'deaths': 'state_deaths'}, inplace = True)\n",
    "    \n",
    "    df = pd.merge(df, state_totals, on = state_grouping_cols)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Calculate non-US Province_State totals\n",
    "def province_totals(df):\n",
    "    \n",
    "    province_grouping_cols = ['Country_Region', 'Province_State', 'date']\n",
    "\n",
    "    province_totals = df.groupby(province_grouping_cols).agg(\n",
    "        {'cases':'sum', 'recovered':'sum', 'deaths':'sum'})\n",
    "    \n",
    "    province_totals.rename(columns = {'cases': 'state_cases',\n",
    "                                  'recovered':'state_recovered', \n",
    "                                  'deaths': 'state_deaths'}, inplace = True)\n",
    "    \n",
    "    df = pd.merge(df, province_totals, on = province_grouping_cols) \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Calculate country totals\n",
    "def country_totals(df):\n",
    "    \n",
    "    country_grouping_cols = ['Country_Region', 'date']\n",
    "    \n",
    "    country_totals = df.groupby(country_grouping_cols).agg(\n",
    "        {'cases':'sum', 'recovered':'sum', 'deaths':'sum'})\n",
    "    \n",
    "    country_totals.rename(columns = {'cases': 'country_cases',\n",
    "                                  'recovered':'country_recovered', \n",
    "                                  'deaths': 'country_deaths'}, inplace = True)\n",
    "    \n",
    "    df = pd.merge(df, country_totals, on = country_grouping_cols) \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_vars, dates = parse_columns(cases1)\n",
    "pre214_df = pd.melt(cases1, id_vars=id_vars, value_vars=dates, value_name=\"cases\", var_name=\"date\",\n",
    ")\n",
    "\n",
    "# melt deaths\n",
    "id_vars, dates = parse_columns(deaths1)\n",
    "deaths_df = pd.melt(deaths1, id_vars=id_vars, value_vars=dates, value_name=\"deaths\")\n",
    "\n",
    "# melt recovered\n",
    "id_vars, dates = parse_columns(recovered1)\n",
    "recovered_df = pd.melt(\n",
    "    recovered1, id_vars=id_vars, value_vars=dates, value_name=\"recovered\"\n",
    ")\n",
    "\n",
    "# join\n",
    "pre214_df[\"deaths\"] = deaths_df.deaths\n",
    "pre214_df[\"recovered\"] = recovered_df.recovered\n",
    "\n",
    "pre214_df['date'] = pd.to_datetime(pre214_df.date)\n",
    "\n",
    "part1 = rename_geog_cols(pre214_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "world1 = part1[part1.Country_Region != 'US'] \n",
    "us1 = part1[part1.Country_Region == 'US']  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre 3/23\n",
    "### This is in 2 groups: 2/15-3/9 and 3/10-3/23, call parts 2 and 3\n",
    "* part 2 is county level...which need to be summed up to get state totals (subset and keep 2/15 - 3/9)\n",
    "* part 3 is state level (subset and keep 3/10-3/23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre323_cases_url= \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/archived_data/archived_time_series/time_series_19-covid-Confirmed_archived_0325.csv\"\n",
    "pre323_deaths_url= \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/archived_data/archived_time_series/time_series_19-covid-Deaths_archived_0325.csv\"\n",
    "pre323_recovered_url= \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/archived_data/archived_time_series/time_series_19-covid-Recovered_archived_0325.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases2 = pd.read_csv(pre323_cases_url)\n",
    "deaths2 = pd.read_csv(pre323_deaths_url)\n",
    "recovered2 = pd.read_csv(pre323_recovered_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_vars, dates = parse_columns(cases2)\n",
    "pre323_df = pd.melt(cases2, id_vars=id_vars, value_vars=dates, value_name=\"cases\", var_name=\"date\",\n",
    ")\n",
    "\n",
    "# melt deaths\n",
    "id_vars, dates = parse_columns(deaths2)\n",
    "deaths_df2 = pd.melt(deaths2, id_vars=id_vars, value_vars=dates, value_name=\"deaths\")\n",
    "\n",
    "# melt recovered\n",
    "id_vars, dates = parse_columns(recovered2)\n",
    "recovered_df2 = pd.melt(\n",
    "    recovered2, id_vars=id_vars, value_vars=dates, value_name=\"recovered\"\n",
    ")\n",
    "\n",
    "# join\n",
    "pre323_df[\"deaths\"] = deaths_df2.deaths\n",
    "pre323_df[\"recovered\"] = recovered_df2.recovered\n",
    "\n",
    "pre323_df['date'] = pd.to_datetime(pre323_df.date)\n",
    "\n",
    "part2 = rename_geog_cols(pre323_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset into part2 and part3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "start2 = '2/15/2020'\n",
    "end2 = '3/9/2020'\n",
    "\n",
    "start3 = '3/10/2020'\n",
    "end3 = '3/23/2020'\n",
    "\n",
    "world2 = part2[(part2.Country_Region != 'US') & (part2.date >= start2) & (part2.date <= end2)] \n",
    "us2 = part2[(part2.Country_Region == 'US') & (part2.date >= start2) & (part2.date <= end2)]  \n",
    "\n",
    "world3 = part2[(part2.Country_Region != 'US') & (part2.date >= start3) & (part2.date <= end3)] \n",
    "us3 = part2[(part2.Country_Region == 'US') & (part2.date >= start3) & (part2.date <= end3)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up each respective part with the right filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# us2 has county-level data, but also state and country-level observations. Drop those.\n",
    "us2 = us2[(us2.Province_State.str.contains(',') == True) | \n",
    "              (us2.Province_State.str.contains('Princess') == True)]\n",
    "\n",
    "us2 = us2[us2.Province_State != 'US']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# us3 has state-level data, but also county and country-level observations. Drop those.\n",
    "us3 = us3[(us3.Province_State.str.contains(',') == False) | \n",
    "              (us3.Province_State.str.contains('Princess') == True)]\n",
    "\n",
    "us3 = us3[us3.Province_State != 'US']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append parts 1-3 together, and do some cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "county = us1.append(us2, sort = False)\n",
    "world = world1.append(world2, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create state_abbrev column, with special case for the cruise ships\n",
    "county['state_abbrev'] = county.Province_State.str.split(', ', expand = True)[1]\n",
    "\n",
    "county['state_abbrev'] = county.apply(lambda row: row.Province_State if row.state_abbrev is None \n",
    "                                else row.state_abbrev, axis = 1)\n",
    "\n",
    "# Create an orig_county columns that stores county-level name. Use to merge later on.\n",
    "county['orig_county'] = county.Province_State\n",
    "\n",
    "# Let's remove \"county\" the name, since sometimes it's Los Angeles County, CA or Los Angeles, CA\n",
    "county['orig_county'] = county.orig_county.str.replace(' County,', ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create state_abbrev column\n",
    "us3['state_abbrev'] = us3.Province_State.map(us_state_abbrev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get state and country totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "county = us_state_totals(county)\n",
    "county = country_totals(county)\n",
    "\n",
    "world = province_totals(world)\n",
    "world = country_totals(world)\n",
    "\n",
    "us3 = us_state_totals(us3)\n",
    "us3 = country_totals(us3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have state/country totals, set the cases, deaths, recovered values to 0 for 3/10-3/23\n",
    "for col in ['cases', 'deaths', 'recovered']:\n",
    "    us3[col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all the US data up to 3/23 together\n",
    "county = county.append(us3, sort = False)\n",
    "\n",
    "# Append all the US and world data together up to 3/23 (before most recent massive schema change)\n",
    "jhu1 = county.append(world, sort = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get rid of duplicates so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def some_cleaning(df):\n",
    "    \n",
    "    df = df.drop_duplicates(subset = ['Country_Region', 'Lat', 'Lon', 'state_abbrev',\n",
    "                                            'date', 'cases', 'deaths', 'recovered'])\n",
    "    \n",
    "    # If there are still duplicates, it's because JHU sometimes did multiple updates a day\n",
    "    # This is ok, we'll keep the higher values for cases, deaths, recovered. \n",
    "    for col in ['cases', 'deaths', 'recovered']:\n",
    "        df[col] = df.groupby(['Province_State', 'Country_Region', \n",
    "                              'Lat', 'Lon', 'date'])[col].transform('max').fillna(0).astype(int)\n",
    "\n",
    "    df = df.drop_duplicates(subset = ['Province_State', 'Country_Region',\n",
    "                                      'Lat', 'Lon', 'date', 'cases', 'deaths', 'recovered'], keep = 'last')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "jhu1 = some_cleaning(jhu1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post 3/23 feature layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_layer_url = \"https://services1.arcgis.com/0MSEUqKaxRlEPj5g/ArcGIS/rest/services/ncov_cases_US/FeatureServer/0/query?where=1%3D1&objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&resultType=none&distance=0.0&units=esriSRUnit_Meter&returnGeodetic=false&outFields=OBJECTID%2C+Province_State%2C+Country_Region%2C+Last_Update%2C+Lat%2C+Long_%2C+Confirmed%2C+Recovered%2C+Deaths%2C+Active%2C+Admin2%2C+FIPS%2C+Combined_Key%2C+Incident_Rate%2C+People_Tested&returnGeometry=true&featureEncoding=esriDefault&multipatchOption=xyFootprint&maxAllowableOffset=&geometryPrecision=&outSR=&datumTransformation=&applyVCSProjection=false&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnExtentOnly=false&returnQueryGeometry=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&returnZ=false&returnM=false&returnExceededLimitFeatures=true&quantizationParameters=&sqlFormat=none&f=pgeojson&token=\"\n",
    "\n",
    "cases326 = gpd.read_file(feature_layer_url)\n",
    "\n",
    "#cases326.to_file(driver = 'GeoJSON', filename = '../data/jhu_feature_layer_3_26_2020.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases325 = gpd.read_file('../data/jhu_feature_layer_3_25_2020.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need help with Last Update column....it's displaying weird ESRI stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases325['date'] = '3/25/2020'\n",
    "cases326['date'] = '3/26/2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append what we have of JHU's new layer so far\n",
    "post323_df = cases325.append(cases326)\n",
    "\n",
    "post323_df['date'] = pd.to_datetime(post323_df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_jhu_post323_schema(df):\n",
    "    \n",
    "    # Rename columns\n",
    "    df.rename(columns = {\"Long_\":\"Lon\", \n",
    "                        \"Confirmed\":\"cases\", \n",
    "                        \"Recovered\":\"recovered\", \n",
    "                        \"Deaths\":\"deaths\", \n",
    "                        \"Admin2\": \"County\"} , inplace = True)  \n",
    "    \n",
    "    \"\"\"\n",
    "    These are the geographic identifiers\n",
    "    Admin2 = County\n",
    "    Province_State = US State\n",
    "    Combined_Key = County, State, Country    \n",
    "    \"\"\"\n",
    "    df['state_abbrev'] = df.Province_State.map(us_state_abbrev)\n",
    "    df['orig_county'] = df.County + \", \" + df.state_abbrev\n",
    "    \n",
    "    # Remove the word \"County\" from orig_county. No difference between Los Angeles County, CA and Los Angeles, CA\n",
    "    df['orig_county'] = df.orig_county.str.replace(' County,', ',')\n",
    "    \n",
    "    # Now change the columns to match with previous schemas\n",
    "    # Province_State will now display county, state abbrev (Los Angeles, CA)\n",
    "    df.Province_State = df.orig_county\n",
    "    \n",
    "    # Add state and country totals (JHU only collecting US county data now, no more non-US country observations)\n",
    "    df = us_state_totals(df)\n",
    "    df = country_totals(df)\n",
    "    \n",
    "    # Drop columns\n",
    "    df = df.drop(columns = ['County', 'Active', 'OBJECTID', 'Last_Update'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "part4 = clean_jhu_post323_schema(post323_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Province_State', 'Country_Region', 'Lat', 'Lon', 'cases', 'recovered',\n",
       "       'deaths', 'FIPS', 'Combined_Key', 'Incident_Rate', 'People_Tested',\n",
       "       'geometry', 'date', 'state_abbrev', 'orig_county', 'state_cases',\n",
       "       'state_recovered', 'state_deaths', 'country_cases', 'country_recovered',\n",
       "       'country_deaths'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Province_State', 'Country_Region', 'Lat', 'Lon', 'date', 'cases',\n",
       "       'deaths', 'recovered', 'state_abbrev', 'orig_county', 'state_cases',\n",
       "       'state_recovered', 'state_deaths', 'country_cases', 'country_recovered',\n",
       "       'country_deaths'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jhu1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have new columns FIPS and Combined_Key in part4. Apply that to jhu1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add FIPS and Combined_Key for combined_df3 before appending\n",
    "fips_key_crosswalk = part4[['orig_county', 'FIPS', 'Combined_Key']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fips_key_crosswalk[fips_key_crosswalk.orig_county.str.contains('k, NY')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "jhu2 = pd.merge(jhu1, fips_key_crosswalk, on = 'orig_county', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "San Antonio, TX         25\n",
       "Madison, WI             25\n",
       "Chicago, IL             25\n",
       "Tempe, AZ               25\n",
       "Seattle, WA             25\n",
       "Boston, MA              25\n",
       "Jackson County, OR      24\n",
       "Jefferson Parish, LA    24\n",
       "Washington, D.C.        24\n",
       "New York County, NY     24\n",
       "Name: Province_State, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jhu2[(jhu2.Country_Region =='US')& (jhu2.FIPS.isna() & \n",
    "                                    (jhu2.Province_State.str.contains(',')))].Province_State.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jhu2.FIPS = jhu2.loc[jhu2.Province_State == 'Jackson County, OR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_county</th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Combined_Key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2902</th>\n",
       "      <td>Jackson, OR</td>\n",
       "      <td>41029</td>\n",
       "      <td>Jackson, Oregon, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6068</th>\n",
       "      <td>Jackson, OR</td>\n",
       "      <td>41029</td>\n",
       "      <td>Jackson, Oregon, US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      orig_county   FIPS         Combined_Key\n",
       "2902  Jackson, OR  41029  Jackson, Oregon, US\n",
       "6068  Jackson, OR  41029  Jackson, Oregon, US"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fips_key_crosswalk[fips_key_crosswalk.orig_county.str.contains(\"Jackson, OR\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_county</th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Combined_Key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3150</th>\n",
       "      <td>District of Columbia, DC</td>\n",
       "      <td>11001</td>\n",
       "      <td>District of Columbia,District of Columbia,US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6315</th>\n",
       "      <td>District of Columbia, DC</td>\n",
       "      <td>11001</td>\n",
       "      <td>District of Columbia,District of Columbia,US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   orig_county   FIPS  \\\n",
       "3150  District of Columbia, DC  11001   \n",
       "6315  District of Columbia, DC  11001   \n",
       "\n",
       "                                      Combined_Key  \n",
       "3150  District of Columbia,District of Columbia,US  \n",
       "6315  District of Columbia,District of Columbia,US  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fips_key_crosswalk[fips_key_crosswalk.orig_county.str.contains(\"DC\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_county</th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Combined_Key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Jefferson, LA</td>\n",
       "      <td>22051</td>\n",
       "      <td>Jefferson, Louisiana, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3239</th>\n",
       "      <td>Jefferson, LA</td>\n",
       "      <td>22051</td>\n",
       "      <td>Jefferson, Louisiana, US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        orig_county   FIPS              Combined_Key\n",
       "71    Jefferson, LA  22051  Jefferson, Louisiana, US\n",
       "3239  Jefferson, LA  22051  Jefferson, Louisiana, US"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fips_key_crosswalk[fips_key_crosswalk.orig_county.str.contains(\"Jefferson, LA\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orig_county</th>\n",
       "      <th>FIPS</th>\n",
       "      <th>Combined_Key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>New York City, NY</td>\n",
       "      <td>36061</td>\n",
       "      <td>New York City, New York, US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4980</th>\n",
       "      <td>New York City, NY</td>\n",
       "      <td>36061</td>\n",
       "      <td>New York City, New York, US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            orig_county   FIPS                 Combined_Key\n",
       "1816  New York City, NY  36061  New York City, New York, US\n",
       "4980  New York City, NY  36061  New York City, New York, US"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fips_key_crosswalk[fips_key_crosswalk.orig_county.str.contains(\"New York\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure FIPS and Combined_Key filled in \n",
    "jhu1.groupby()\n",
    "\n",
    "# Check for duplicates, which come from slightly different spellings of county names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append jhu1 and part4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df3[(combined_df3.Country_Region=='US') & (combined_df3.Province_State == '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(combined_df3, fips_key_crosswalk, on = ['orig_county']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df4 = combined_df3.append(post323_df).sort_values(['Country_Region', \n",
    "                                                            'Province_State', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add FIPS\n",
    "\n",
    "combined_df4 = pd.merge(combined_df4.drop(columns = 'FIPS'), county_fips_crosswalk, \n",
    "                        on = 'orig_county', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Combined_Key for US\n",
    "us2 = combined_df4[combined_df4.Country_Region == \"US\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_key_crosswalk = us2[['FIPS', 'Combined_Key']][us2.FIPS.notna()].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us2 = pd.merge(us2.drop(columns = 'Combined_Key'), combined_key_crosswalk, on = 'FIPS', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us2[us2.Combined_Key.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us2['Combined_Key'] = us2.Province_State.str.split(',').str[0] + \", \" + us2.state_abbrev + \", \" + us2.Country_Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add FIPS, state_abbrev, Combined_Key columns\n",
    "us['state_abbrev'] = us.Province_State.map(us_state_abbrev)\n",
    "\n",
    "county_fips_crosswalk = cases3[['orig_county', 'FIPS']].drop_duplicates()\n",
    "\n",
    "us2 = pd.merge(us, county_fips_crosswalk, on = 'orig_county')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df2 = us2.append(world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append combined_df2 and cases3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geoalchemy2 import Geometry, WKTElement\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Fill in geometry - may not be necessary because we ESRI uses Lat, Lon columns\n",
    "srid = 4326\n",
    "df = df.dropna(subset=['Lat', 'Lon'])\n",
    "df[\"geometry\"] = df.apply(\n",
    "    lambda x: WKTElement(Point(x.Lon, x.Lat).wkt, srid=srid), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change date column to be datetime....Ian has code for this"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
