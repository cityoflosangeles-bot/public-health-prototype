{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stitch JHU data through various schema changes\n",
    "* Reshape\n",
    "* See what columns we need to derive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/rogerallen/1583593\n",
    "us_state_abbrev = {\n",
    "    'Alabama': 'AL',\n",
    "    'Alaska': 'AK',\n",
    "    'American Samoa': 'AS',\n",
    "    'Arizona': 'AZ',\n",
    "    'Arkansas': 'AR',\n",
    "    'California': 'CA',\n",
    "    'Colorado': 'CO',\n",
    "    'Connecticut': 'CT',\n",
    "    'Delaware': 'DE',\n",
    "    'District of Columbia': 'DC',\n",
    "    'Florida': 'FL',\n",
    "    'Georgia': 'GA',\n",
    "    'Guam': 'GU',\n",
    "    'Hawaii': 'HI',\n",
    "    'Idaho': 'ID',\n",
    "    'Illinois': 'IL',\n",
    "    'Indiana': 'IN',\n",
    "    'Iowa': 'IA',\n",
    "    'Kansas': 'KS',\n",
    "    'Kentucky': 'KY',\n",
    "    'Louisiana': 'LA',\n",
    "    'Maine': 'ME',\n",
    "    'Maryland': 'MD',\n",
    "    'Massachusetts': 'MA',\n",
    "    'Michigan': 'MI',\n",
    "    'Minnesota': 'MN',\n",
    "    'Mississippi': 'MS',\n",
    "    'Missouri': 'MO',\n",
    "    'Montana': 'MT',\n",
    "    'Nebraska': 'NE',\n",
    "    'Nevada': 'NV',\n",
    "    'New Hampshire': 'NH',\n",
    "    'New Jersey': 'NJ',\n",
    "    'New Mexico': 'NM',\n",
    "    'New York': 'NY',\n",
    "    'North Carolina': 'NC',\n",
    "    'North Dakota': 'ND',\n",
    "    'Northern Mariana Islands':'MP',\n",
    "    'Ohio': 'OH',\n",
    "    'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR',\n",
    "    'Pennsylvania': 'PA',\n",
    "    'Puerto Rico': 'PR',\n",
    "    'Rhode Island': 'RI',\n",
    "    'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD',\n",
    "    'Tennessee': 'TN',\n",
    "    'Texas': 'TX',\n",
    "    'Utah': 'UT',\n",
    "    'Vermont': 'VT',\n",
    "    'Virgin Islands': 'VI',\n",
    "    'Virginia': 'VA',\n",
    "    'Washington': 'WA',\n",
    "    'West Virginia': 'WV',\n",
    "    'Wisconsin': 'WI',\n",
    "    'Wyoming': 'WY',\n",
    "    # Add some other ones we found applicable\n",
    "    'US Virgin Islands': 'VI', \n",
    "    'United States Virgin Islands': 'VI',\n",
    "    'Grand Princess': 'Grand Princess',\n",
    "    'Diamond Princess': 'Diamond Princess', \n",
    "    'From Diamond Princess': 'Diamond Princess', \n",
    "    'Diamond Princess cruise ship': 'Diamond Princess'\n",
    "}\n",
    "\n",
    "# reverse the dict\n",
    "abbrev_us_state = dict(map(reversed, us_state_abbrev.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre 2/14/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre214_cases_url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/archived_data/archived_time_series/time_series_2019-ncov-Confirmed.csv\"\n",
    "pre214_deaths_url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/archived_data/archived_time_series/time_series_2019-ncov-Deaths.csv\"\n",
    "pre214_recovered_url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/archived_data/archived_time_series/time_series_2019-ncov-Recovered.csv\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases1 = pd.read_csv(pre214_cases_url)\n",
    "deaths1 = pd.read_csv(pre214_deaths_url)\n",
    "recovered1 = pd.read_csv(pre214_recovered_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modified helper function, since columns are datetime, will extract date portion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_columns(df):\n",
    "    \"\"\"\n",
    "    quick helper function to parse columns into values\n",
    "    uses for pd.melt\n",
    "    \"\"\"\n",
    "    df.columns = df.columns.str.split(' ').str[0]\n",
    "    columns = list(df.columns)\n",
    "    id_vars, dates = [], []\n",
    "\n",
    "    for c in columns:\n",
    "        if c.endswith(\"20\"):\n",
    "            dates.append(c)\n",
    "        else:\n",
    "            id_vars.append(c)\n",
    "    return id_vars, dates\n",
    "\n",
    "# Rename geography columns to be the same as future schemas\n",
    "def rename_geog_cols(df):\n",
    "    df.rename(columns = {'Country/Region':'Country_Region', \n",
    "                         'Province/State': 'Province_State', \n",
    "                         'Long': 'Lon'}, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_vars, dates = parse_columns(cases1)\n",
    "pre214_df = pd.melt(cases1, id_vars=id_vars, value_vars=dates, value_name=\"cases\", var_name=\"date\",\n",
    ")\n",
    "\n",
    "# melt deaths\n",
    "id_vars, dates = parse_columns(deaths1)\n",
    "deaths_df = pd.melt(deaths1, id_vars=id_vars, value_vars=dates, value_name=\"deaths\")\n",
    "\n",
    "# melt recovered\n",
    "id_vars, dates = parse_columns(recovered1)\n",
    "recovered_df = pd.melt(\n",
    "    recovered1, id_vars=id_vars, value_vars=dates, value_name=\"recovered\"\n",
    ")\n",
    "\n",
    "# join\n",
    "pre214_df[\"deaths\"] = deaths_df.deaths\n",
    "pre214_df[\"recovered\"] = recovered_df.recovered\n",
    "\n",
    "pre214_df['date'] = pd.to_datetime(pre214_df.date)\n",
    "\n",
    "pre214_df = rename_geog_cols(pre214_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre 3/23\n",
    "* will have overlap with pre 2/14...let's see how this resolves itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre323_cases_url= \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/archived_data/archived_time_series/time_series_19-covid-Confirmed_archived_0325.csv\"\n",
    "pre323_deaths_url= \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/archived_data/archived_time_series/time_series_19-covid-Deaths_archived_0325.csv\"\n",
    "pre323_recovered_url= \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/archived_data/archived_time_series/time_series_19-covid-Recovered_archived_0325.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases2 = pd.read_csv(pre323_cases_url)\n",
    "deaths2 = pd.read_csv(pre323_deaths_url)\n",
    "recovered2 = pd.read_csv(pre323_recovered_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_vars, dates = parse_columns(cases2)\n",
    "pre323_df = pd.melt(cases2, id_vars=id_vars, value_vars=dates, value_name=\"cases\", var_name=\"date\",\n",
    ")\n",
    "\n",
    "# melt deaths\n",
    "id_vars, dates = parse_columns(deaths2)\n",
    "deaths_df2 = pd.melt(deaths2, id_vars=id_vars, value_vars=dates, value_name=\"deaths\")\n",
    "\n",
    "# melt recovered\n",
    "id_vars, dates = parse_columns(recovered2)\n",
    "recovered_df2 = pd.melt(\n",
    "    recovered2, id_vars=id_vars, value_vars=dates, value_name=\"recovered\"\n",
    ")\n",
    "\n",
    "# join\n",
    "pre323_df[\"deaths\"] = deaths_df2.deaths\n",
    "pre323_df[\"recovered\"] = recovered_df2.recovered\n",
    "\n",
    "pre323_df['date'] = pd.to_datetime(pre323_df.date)\n",
    "\n",
    "pre323_df = rename_geog_cols(pre323_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre 3/23 data is of 2 types\n",
    "* Pre 3/10 county level...which need to be summed up to get state totals\n",
    "* 3/10-3/23 state level...lose county level except for SCAG region\n",
    "\n",
    "### Combine pre214_df and pre310_df and get rid of duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The US data shows county (with all zeros), state, and country counts.\n",
    "# Filter out county because they're all zeros.\n",
    "# Filter out country total, or else we double count.\n",
    "us_pre323 = pre323_df[(pre323_df.Country_Region == 'US') & \n",
    "                      (pre323_df.Province_State.str.contains(',') == False) &\n",
    "                     (pre323_df.Province_State != 'US')]\n",
    "\n",
    "world_pre323 = pre323_df[pre323_df.Country_Region != 'US']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add state abbrev\n",
    "us_pre323['state_abbrev'] = us_pre323.Province_State.map(us_state_abbrev)\n",
    "\n",
    "# There are some duplicates, such as US Virgin Islands or Virgin Islands. Drop as long as state abbrev is the same.\n",
    "us_pre323 = us_pre323.drop_duplicates(subset = ['Country_Region', 'Lat', 'Lon', 'state_abbrev',\n",
    "                                               'date', 'cases', 'deaths', 'recovered'])\n",
    "\n",
    "pre323_df2 = us_pre323.append(world_pre323)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre310_df = pre323_df2[pre323_df2.date <= '3/10/20']\n",
    "post310_df = pre323_df2[pre323_df2.date > '3/10/20']\n",
    "\n",
    "combined_df1 = pre214_df.append(pre310_df).drop_duplicates().sort_values(['Country_Region', \n",
    "                                                                          'Province_State', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there are still duplicates, it's because JHU sometimes did multiple updates a day\n",
    "# This is ok, we'll keep the higher values for cases, deaths, recovered. \n",
    "for col in ['cases', 'deaths', 'recovered']:\n",
    "    combined_df1[col] = combined_df1.groupby(['Province_State', 'Country_Region', \n",
    "                                            'Lat', 'Lon', 'date'])[col].transform('max').fillna(0).astype(int)\n",
    "\n",
    "\n",
    "combined_df1 = combined_df1.drop_duplicates(subset = ['Province_State', 'Country_Region',\n",
    "                                                   'Lat', 'Lon', 'date',\n",
    "                                                   'cases', 'deaths', 'recovered'], keep = 'last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the US, since Province/State contains county and state info, we need to derive our own state totals\n",
    "us1 = combined_df1[combined_df1.Country_Region == \"US\"]\n",
    "world1 = combined_df1[combined_df1.Country_Region != \"US\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some functions we'll use to get totals\n",
    "# Calculate US State totals\n",
    "def us_state_totals(df):\n",
    "    \n",
    "    state_grouping_cols = ['Country_Region', 'state_abbrev', 'date']\n",
    "    \n",
    "    state_totals = df.groupby(state_grouping_cols).agg(\n",
    "        {'cases':'sum', 'recovered':'sum', 'deaths':'sum'})\n",
    "    \n",
    "    state_totals.rename(columns = {'cases': 'state_cases',\n",
    "                                  'recovered':'state_recovered', \n",
    "                                  'deaths': 'state_deaths'}, inplace = True)\n",
    "    \n",
    "    df = pd.merge(df, state_totals, on = state_grouping_cols)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Calculate non-US Province_State totals\n",
    "def province_totals(df):\n",
    "    \n",
    "    province_grouping_cols = ['Country_Region', 'Province_State', 'date']\n",
    "\n",
    "    province_totals = df.groupby(province_grouping_cols).agg(\n",
    "        {'cases':'sum', 'recovered':'sum', 'deaths':'sum'})\n",
    "    \n",
    "    province_totals.rename(columns = {'cases': 'state_cases',\n",
    "                                  'recovered':'state_recovered', \n",
    "                                  'deaths': 'state_deaths'}, inplace = True)\n",
    "    \n",
    "    df = pd.merge(df, province_totals, on = province_grouping_cols) \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Calculate country totals\n",
    "def country_totals(df):\n",
    "    \n",
    "    country_grouping_cols = ['Country_Region', 'date']\n",
    "    \n",
    "    country_totals = df.groupby(country_grouping_cols).agg(\n",
    "        {'cases':'sum', 'recovered':'sum', 'deaths':'sum'})\n",
    "    \n",
    "    country_totals.rename(columns = {'cases': 'country_cases',\n",
    "                                  'recovered':'country_recovered', \n",
    "                                  'deaths': 'country_deaths'}, inplace = True)\n",
    "    \n",
    "    df = pd.merge(df, country_totals, on = country_grouping_cols) \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add state/province/country totals\n",
    "us1 = us_state_totals(us1)\n",
    "us1 = country_totals(us1)\n",
    "\n",
    "world1 = province_totals(world1)\n",
    "world1 = country_totals(world1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append US with rest of the world\n",
    "combined_df2 = us1.append(world1)\n",
    "\n",
    "combined_df2['Province_State'] = combined_df2.Province_State.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correctly append combined_df2 and post310_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the post310_df first to make sure we have all the columns we need\n",
    "# Derive the state and country total columns\n",
    "# Post 3/10 data is all at the state level for US & rest of the world\n",
    "for col in ['cases', 'deaths', 'recovered']:\n",
    "    new_col = f\"state_{col}\"\n",
    "    post310_df[new_col] = post310_df[col]\n",
    "    \n",
    "    \n",
    "post310_df = country_totals(post310_df)\n",
    "\n",
    "\n",
    "# Also, set the Province_State column to blanks, because Province_State for the US will display county, state.\n",
    "post310_df['Province_State'] = post310_df.apply(lambda row: \"\" if row.Country_Region == \"US\"\n",
    "                                                else row.Province_State, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df3 = combined_df2.append(post310_df, sort = False).sort_values(['Country_Region', \n",
    "                                                                          'Province_State', 'date'])\n",
    "combined_df3.Province_State = combined_df3.Province_State.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post 3/23 feature layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_layer_url = \"https://services1.arcgis.com/0MSEUqKaxRlEPj5g/ArcGIS/rest/services/ncov_cases_US/FeatureServer/0/query?where=1%3D1&objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&resultType=none&distance=0.0&units=esriSRUnit_Meter&returnGeodetic=false&outFields=OBJECTID%2C+Province_State%2C+Country_Region%2C+Last_Update%2C+Lat%2C+Long_%2C+Confirmed%2C+Recovered%2C+Deaths%2C+Active%2C+Admin2%2C+FIPS%2C+Combined_Key%2C+Incident_Rate%2C+People_Tested&returnGeometry=true&featureEncoding=esriDefault&multipatchOption=xyFootprint&maxAllowableOffset=&geometryPrecision=&outSR=&datumTransformation=&applyVCSProjection=false&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnExtentOnly=false&returnQueryGeometry=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&returnZ=false&returnM=false&returnExceededLimitFeatures=true&quantizationParameters=&sqlFormat=none&f=pgeojson&token=\"\n",
    "\n",
    "cases326 = gpd.read_file(feature_layer_url)\n",
    "\n",
    "#cases326.to_file(driver = 'GeoJSON', filename = '../data/jhu_feature_layer_3_26_2020.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases325 = gpd.read_file('../data/jhu_feature_layer_3_25_2020.geojson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need help with Last Update column....it's displaying weird ESRI stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases325['date'] = '3/25/2020'\n",
    "cases326['date'] = '3/26/2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append what we have of JHU's new layer so far\n",
    "post323_df = cases325.append(cases326)\n",
    "\n",
    "post323_df['date'] = pd.to_datetime(post323_df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_jhu_post323_schema(df):\n",
    "    \n",
    "    # Rename columns\n",
    "    df.rename(columns = {\"Long_\":\"Lon\", \n",
    "                        \"Confirmed\":\"cases\", \n",
    "                        \"Recovered\":\"recovered\", \n",
    "                        \"Deaths\":\"deaths\", \n",
    "                        \"Admin2\": \"County\"} , inplace = True)  \n",
    "    \n",
    "    \"\"\"\n",
    "    These are the geographic identifiers\n",
    "    Admin2 = County\n",
    "    Province_State = US State\n",
    "    Combined_Key = County, State, Country    \n",
    "    \"\"\"\n",
    "    df['state_abbrev'] = df.Province_State.map(us_state_abbrev)\n",
    "    df['orig_county'] = df.County + \", \" + df.state_abbrev\n",
    "    \n",
    "    \n",
    "    # Now change the columns to match with previous schemas\n",
    "    # Province_State will now display county, state abbrev (Los Angeles, CA)\n",
    "    df.Province_State = df.orig_county\n",
    "    \n",
    "    # Add state and country totals (JHU only collecting US county data now, no more non-US country observations)\n",
    "    df = us_state_totals(df)\n",
    "    df = country_totals(df)\n",
    "    \n",
    "    # Drop columns\n",
    "    df = df.drop(columns = ['County', 'Active'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post323_df = clean_jhu_post323_schema(post323_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append combined_df3 and post323_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df4 = combined_df3.append(post323_df).sort_values(['Country_Region', \n",
    "                                                            'Province_State', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add FIPS\n",
    "county_fips_crosswalk = post323_df[['orig_county', 'FIPS']].drop_duplicates(subset = ['FIPS'], keep = 'last')\n",
    "\n",
    "combined_df4 = pd.merge(combined_df4.drop(columns = 'FIPS'), county_fips_crosswalk, \n",
    "                        on = 'orig_county', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Combined_Key for US\n",
    "us2 = combined_df4[combined_df4.Country_Region == \"US\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_key_crosswalk = us2[['FIPS', 'Combined_Key']][us2.FIPS.notna()].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us2 = pd.merge(us2.drop(columns = 'Combined_Key'), combined_key_crosswalk, on = 'FIPS', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us2[us2.Combined_Key.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us2['Combined_Key'] = us2.Province_State.str.split(',').str[0] + \", \" + us2.state_abbrev + \", \" + us2.Country_Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add FIPS, state_abbrev, Combined_Key columns\n",
    "us['state_abbrev'] = us.Province_State.map(us_state_abbrev)\n",
    "\n",
    "county_fips_crosswalk = cases3[['orig_county', 'FIPS']].drop_duplicates()\n",
    "\n",
    "us2 = pd.merge(us, county_fips_crosswalk, on = 'orig_county')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df2 = us2.append(world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append combined_df2 and cases3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geoalchemy2 import Geometry, WKTElement\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Fill in geometry - may not be necessary because we ESRI uses Lat, Lon columns\n",
    "srid = 4326\n",
    "df = df.dropna(subset=['Lat', 'Lon'])\n",
    "df[\"geometry\"] = df.apply(\n",
    "    lambda x: WKTElement(Point(x.Lon, x.Lat).wkt, srid=srid), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change date column to be datetime....Ian has code for this"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
